{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, findspark\n",
    "home=os.path.expanduser(\"~\")\n",
    "findspark.init(os.path.join(home,\"Developer\",\"spark-2.0.0-bin-hadoop2.6\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'2.0.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myList=[('1','kim, js',170),\n",
    "        ('1','lee, sm', 175),\n",
    "        ('2','lim, yg',180),\n",
    "        ('2','lee',170)]\n",
    "myDf=spark.createDataFrame(myList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n",
      "[Row(_1=u'1', _2=u'kim, js', _3=170)]\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()\n",
    "print myDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(year=u'1', name=u'kim, js', height=170)]\n"
     ]
    }
   ],
   "source": [
    "print spark.createDataFrame(myList, ['year','name','height']).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|name|      item|\n",
      "+----+----------+\n",
      "| kim|  espresso|\n",
      "| lee|     latte|\n",
      "| lee| americano|\n",
      "| lim|  affocato|\n",
      "| kim|long black|\n",
      "| lee|  espresso|\n",
      "| lee|     latte|\n",
      "| lim| americano|\n",
      "| kim|  affocato|\n",
      "| lee|long black|\n",
      "| lee|  espresso|\n",
      "| lim|     latte|\n",
      "| kim| americano|\n",
      "| lee|  affocato|\n",
      "| lee|long black|\n",
      "| lim|  espresso|\n",
      "| kim|     latte|\n",
      "| lee| americano|\n",
      "| lee|  affocato|\n",
      "| lim|long black|\n",
      "+----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names = [\"kim\",\"lee\",\"lee\",\"lim\"]\n",
    "items = [\"espresso\",\"latte\",\"americano\",\"affocato\",\"long black\",\"macciato\"]\n",
    "df = spark.createDataFrame([(names[i%4], items[i%5]) for i in range(100)],\\\n",
    "                           [\"name\",\"item\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|substring(item, 1, 3)|\n",
      "+---------------------+\n",
      "|                  esp|\n",
      "|                  lat|\n",
      "+---------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.item.substr(1, 3)).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|3chars|\n",
      "+------+\n",
      "|   esp|\n",
      "|   lat|\n",
      "|   ame|\n",
      "+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.item.substr(1, 3).alias(\"3chars\")).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row1:  1 kim, js\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.Row"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "Person = Row('year','name', 'height')\n",
    "row1=Person('1','kim, js',170)\n",
    "print \"row1: \",row1.year, row1.name\n",
    "type(Person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myRows = [row1,\n",
    "          Person('1','lee, sm', 175),\n",
    "          Person('2','lim, yg',180),\n",
    "          Person('2','lee',170)]\n",
    "\n",
    "myDf=spark.createDataFrame(myRows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: long (nullable = true)\n",
      "\n",
      "None\n",
      "+----+-------+------+\n",
      "|year|   name|height|\n",
      "+----+-------+------+\n",
      "|   1|kim, js|   170|\n",
      "|   1|lee, sm|   175|\n",
      "|   2|lim, yg|   180|\n",
      "|   2|    lee|   170|\n",
      "+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print myDf.printSchema()\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: integer (nullable = true)\n",
      "\n",
      "+----+-------+------+\n",
      "|year|   name|height|\n",
      "+----+-------+------+\n",
      "|   1|kim, js|   170|\n",
      "|   1|lee, sm|   175|\n",
      "|   2|lim, yg|   180|\n",
      "|   2|    lee|   170|\n",
      "+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "mySchema=StructType([\n",
    "    StructField(\"year\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"height\", IntegerType(), True)\n",
    "])\n",
    "myDf=spark.createDataFrame(myRows, mySchema)\n",
    "myDf.printSchema()\n",
    "myDf.take(1)\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "myList=[('1','kim, js',170),('1','lee, sm', 175),('2','lim, yg',180),('2','lee',170)]\n",
    "myRdd = spark.sparkContext.parallelize(myList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n",
      "+---+-------+---+\n",
      "| _1|     _2| _3|\n",
      "+---+-------+---+\n",
      "|  1|kim, js|170|\n",
      "|  1|lee, sm|175|\n",
      "|  2|lim, yg|180|\n",
      "|  2|    lee|170|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rddDf=myRdd.toDF()\n",
    "rddDf.printSchema()\n",
    "rddDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| _1|     _2|\n",
      "+---+-------+\n",
      "|  1|kim, js|\n",
      "|  2|    lee|\n",
      "+---+-------+\n",
      "\n",
      "+---+-------+\n",
      "| _1|max(_3)|\n",
      "+---+-------+\n",
      "|  1|    175|\n",
      "|  2|    180|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rddDf .select([rddDf._1, rddDf._2]).where(rddDf._3 < 175)\\\n",
    "   .show()\n",
    "rddDf.groupby(rddDf._1).max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- height: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(height=170, name=u'kim, js', year=1)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_myRdd=myRdd.map(lambda x:Row(year=int(x[0]),name=x[1],height=int(x[2])))\n",
    "_myDf=spark.createDataFrame(_myRdd)\n",
    "_myDf.printSchema()\n",
    "_myDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=10, name='js1'), Row(age=20, name='js2')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, IntegerType, TimestampType\n",
    "r1=Row(name=\"js1\",age=10)\n",
    "r2=Row(name=\"js2\",age=20)\n",
    "_myRdd=spark.sparkContext.parallelize([r1,r2])\n",
    "_myRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      "\n",
      "+---+----+------+\n",
      "| id|name|height|\n",
      "+---+----+------+\n",
      "|  1| kim|  50.0|\n",
      "|  2| lee|  60.0|\n",
      "|  3|park|  70.0|\n",
      "+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "myRdd=spark.sparkContext.parallelize([(1, 'kim', 50.0), (2, 'lee', 60.0), (3, 'park', 70.0)])\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"height\", DoubleType(), True)\n",
    "])\n",
    "_myDf = spark.createDataFrame(myRdd, schema)\n",
    "_myDf.printSchema()\n",
    "_myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>name</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>kim, js</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>lee, sm</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>lim, yg</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>lee</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  year     name  height\n",
       "0    1  kim, js     170\n",
       "1    1  lee, sm     175\n",
       "2    2  lim, yg     180\n",
       "3    2      lee     170"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myDf.write.format('com.databricks.spark.csv').save(os.path.join('data','_myDf.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS\r\n",
      "part-r-00000-96fdb767-dd4f-4d5e-af79-dd7c523e6578.csv\r\n",
      "part-r-00001-96fdb767-dd4f-4d5e-af79-dd7c523e6578.csv\r\n",
      "part-r-00002-96fdb767-dd4f-4d5e-af79-dd7c523e6578.csv\r\n",
      "part-r-00003-96fdb767-dd4f-4d5e-af79-dd7c523e6578.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/_myDf.csv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myDf.toPandas().to_csv(os.path.join('data','myDf.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_jfname=os.path.join('src','ds_twitter_seoul_3.json')\n",
    "with open(_jfname, 'rb') as f:\n",
    "    data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = map(lambda x: x.rstrip(), data)\n",
    "#공백기준으로 오른쪽 제거 right trim 같은느낌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_json_str = \"[\" + ','.join(data) + \"]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_df = pd.read_json(data_json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contributors                    0\n",
      "coordinates                    14\n",
      "created_at                   2000\n",
      "entities                     2000\n",
      "extended_entities             973\n",
      "favorite_count               2000\n",
      "favorited                    2000\n",
      "geo                            14\n",
      "id                           2000\n",
      "id_str                       2000\n",
      "in_reply_to_screen_name        40\n",
      "in_reply_to_status_id          39\n",
      "in_reply_to_status_id_str      39\n",
      "in_reply_to_user_id            40\n",
      "in_reply_to_user_id_str        40\n",
      "is_quote_status              2000\n",
      "lang                         2000\n",
      "metadata                     2000\n",
      "place                          19\n",
      "possibly_sensitive           1117\n",
      "quoted_status                   3\n",
      "quoted_status_id                7\n",
      "quoted_status_id_str            7\n",
      "retweet_count                2000\n",
      "retweeted                    2000\n",
      "retweeted_status             1834\n",
      "source                       2000\n",
      "text                         2000\n",
      "truncated                    2000\n",
      "user                         2000\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print data_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    873793265719324674\n",
       "1    873793260711419904\n",
       "2    873793260568928256\n",
       "3    873793251093827584\n",
       "4    873793247004344320\n",
       "5    873793246639423488\n",
       "6    873793246295609345\n",
       "7    873793246186668033\n",
       "8    873793245796368386\n",
       "9    873793244450045952\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df['id'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(age=29, name=u'Michael'),\n",
       " Row(age=30, name=u'Andy'),\n",
       " Row(age=19, name=u'Justin')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "cfile= os.path.join(os.environ[\"SPARK_HOME\"],\\\n",
    "           \"examples/src/main/resources/people.txt\")\n",
    "lines = spark.sparkContext.textFile(cfile)\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "people = parts.map(lambda p: Row(name=p[0], age=int(p[1].strip())))\n",
    "\n",
    "_myDf = spark.createDataFrame(people)\n",
    "_myDf.printSchema()\n",
    "_myDf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/ds_spark.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark.csv\n",
    "1,2,3,4\n",
    "11,22,33,44\n",
    "111,222,333,444"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "|  1|  2|  3|  4|\n",
      "+---+---+---+---+\n",
      "| 11| 22| 33| 44|\n",
      "|111|222|333|444|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('com.databricks.spark.csv')\\\n",
    "    .options(header='true', inferschema='true').load('data/ds_spark.csv')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.658985,  4.285136])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array([float(x) for x in '1.658985\t4.285136'.split('\\t')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.658985,  4.285136])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array([float(x) for x in '1.658985 4.285136'.split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "rdd=spark.sparkContext\\\n",
    "    .textFile(os.path.join('data','ds_spark_heightweight.txt'))\n",
    "\n",
    "tRdd=rdd.map(lambda x:x.split('\\t'))\n",
    "tDf=spark.createDataFrame(tRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1=u'1', _2=u'65.78', _3=u'112.99')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tDf=tDf.withColumn(\"id\",tDf['_1'].cast(\"integer\")).drop('_1')\n",
    "tDf=tDf.withColumn(\"height\",tDf['_2'].cast(\"double\")).drop('_2')\n",
    "tDf=tDf.withColumn(\"weight\",tDf['_3'].cast(\"double\")).drop('_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mySchema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"weight\", DoubleType(), True),\n",
    "    StructField(\"height\", DoubleType(), True)\n",
    "])\n",
    "myDf=spark.createDataFrame(rdd, mySchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, height=65.78, weight=112.99)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 65.78, 112.99]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import numpy as np\n",
    "#myRdd=rdd.map(lambda line:np.array([float(x) for x in line.split('\\t')]))\n",
    "tRdd=rdd.map(lambda line:[float(x) for x in line.split('\\t')])\n",
    "tRdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id=1.0, weight=65.78, height=112.99)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "tDf=spark.createDataFrame(tRdd,[\"id\",\"weight\",\"height\"])\n",
    "tDf.printSchema()\n",
    "tDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 65.78  71.52  69.4   68.22  67.79]\n",
      "[ 112.99  136.49  153.03  142.34  144.3 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "_weightRdd=tDf.rdd.map(lambda fields:fields[1]).collect()\n",
    "_heightRdd=tDf.rdd.map(lambda fields:fields[2]).collect()\n",
    "print np.array(_weightRdd)[:5]\n",
    "print np.array(_heightRdd)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c7afc90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.array(_weightRdd), np.array(_heightRdd),'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jfile= os.path.join(os.environ[\"SPARK_HOME\"],\\\n",
    "           \"examples/src/main/resources/people.json\")\n",
    "\n",
    "_myDf= spark.read.json(jfile)\n",
    "_myDf.filter(_myDf['age'] > 21).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
