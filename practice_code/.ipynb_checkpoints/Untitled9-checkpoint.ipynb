{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, findspark\n",
    "home=os.path.expanduser(\"~\")\n",
    "findspark.init(os.path.join(home,\"Developer\",\"spark-2.0.0-bin-hadoop2.6\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'2.0.0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#_fp=os.path.join(spark_home,\"data/mllib/sample_svm_data.txt\")\n",
    "_fp=os.path.join(os.environ[\"SPARK_HOME\"],\\\n",
    "                 'data','mllib','sample_svm_data.txt')\n",
    "print os.path.isfile(_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0 2.52078447201548 0 0 0 2.004684436494304 2.000347299268466 0 2.228387042742021 2.228387042742023 0 0 0 0 0 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_f=open(_fp,'r')\n",
    "_lines=_f.readlines()\n",
    "print _lines[0]\n",
    "_f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 0.0,\n",
       " 2.52078447201548,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.004684436494304,\n",
       " 2.000347299268466,\n",
       " 0.0,\n",
       " 2.228387042742021,\n",
       " 2.228387042742023,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rdd=spark.sparkContext.textFile(_fp)\\\n",
    "    .map(lambda line: [float(x) for x in line.split(' ')])\n",
    "_rdd.take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, [0.0,2.52078447202,0.0,0.0,0.0,2.00468443649,2.00034729927,0.0,2.22838704274,2.22838704274,0.0,0.0,0.0,0.0,0.0,0.0])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "_trainRdd0=_rdd.map(lambda line:LabeledPoint(line[0], line[1:]))\n",
    "_trainRdd0.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, [0.0,2.52078447202,0.0,0.0,0.0,2.00468443649,2.00034729927,0.0,2.22838704274,2.22838704274,0.0,0.0,0.0,0.0,0.0,0.0])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_trainRdd=spark.sparkContext.textFile(_fp)\\\n",
    "    .map(lambda line: [float(x) for x in line.split(' ')])\\\n",
    "    .map(lambda p:LabeledPoint(p[0], p[1:]))\n",
    "_trainRdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, [0.0,2.52078447202,0.0,0.0,0.0,2.00468443649,2.00034729927,0.0,2.22838704274,2.22838704274,0.0,0.0,0.0,0.0,0.0,0.0])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def createLP(line):\n",
    "    p = [float(x) for x in line.split(' ')]\n",
    "    return LabeledPoint(p[0], p[1:])\n",
    "\n",
    "_rdd=spark.sparkContext.textFile(_fp)\n",
    "trainRdd = _rdd.map(createLP)\n",
    "\n",
    "trainRdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        ['No','young', 'false', 'false', 'fair'],\n",
    "        ['No','young', 'false', 'false', 'good'],\n",
    "        ['Yes','young', 'true', 'false', 'good'],\n",
    "        ['Yes','young', 'true', 'true', 'fair'],\n",
    "        ['No','young', 'false', 'false', 'fair'],\n",
    "        ['No','middle', 'false', 'false', 'fair'],\n",
    "        ['No','middle', 'false', 'false', 'good'],\n",
    "        ['Yes','middle', 'true', 'true', 'good'],\n",
    "        ['Yes','middle', 'false', 'true', 'excellent'],\n",
    "        ['Yes','middle', 'false', 'true', 'excellent'],\n",
    "        ['Yes','old', 'false', 'true', 'excellent'],\n",
    "        ['Yes','old', 'false', 'true', 'good'],\n",
    "        ['Yes','old', 'true', 'false', 'good'],\n",
    "        ['Yes','old', 'true', 'false', 'excellent'],\n",
    "        ['No','old', 'false', 'false', 'fair'],\n",
    "    ],\n",
    "    ['cls','age','f1','f2','f3']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cls: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- f1: string (nullable = true)\n",
      " |-- f2: string (nullable = true)\n",
      " |-- f3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "#from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "clsIndexer = StringIndexer(inputCol=\"cls\", outputCol=\"label\")\n",
    "i1Indexer = StringIndexer(inputCol=\"age\", outputCol=\"i1\")\n",
    "i2Indexer = StringIndexer(inputCol=\"f1\", outputCol=\"i2\")\n",
    "i3Indexer = StringIndexer(inputCol=\"f2\", outputCol=\"i3\")\n",
    "i4Indexer = StringIndexer(inputCol=\"f3\", outputCol=\"i4\")\n",
    "va = VectorAssembler(inputCols=[\"i1\",\"i2\",\"i3\",\"i4\"],outputCol=\"features\")\n",
    "#df3 = va.transform(df2)\n",
    "\n",
    "pipeline = Pipeline(stages=[clsIndexer,i1Indexer,i2Indexer,i3Indexer,i4Indexer,va])\n",
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(df)\n",
    "df2 = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cls: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- f1: string (nullable = true)\n",
      " |-- f2: string (nullable = true)\n",
      " |-- f3: string (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      " |-- i1: double (nullable = true)\n",
      " |-- i2: double (nullable = true)\n",
      " |-- i3: double (nullable = true)\n",
      " |-- i4: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+---+------+-----+-----+---------+-----+---+---+---+---+-----------------+\n",
      "|cls|   age|   f1|   f2|       f3|label| i1| i2| i3| i4|         features|\n",
      "+---+------+-----+-----+---------+-----+---+---+---+---+-----------------+\n",
      "| No| young|false|false|     fair|  1.0|0.0|0.0|0.0|1.0|    (4,[3],[1.0])|\n",
      "| No| young|false|false|     good|  1.0|0.0|0.0|0.0|0.0|        (4,[],[])|\n",
      "|Yes| young| true|false|     good|  0.0|0.0|1.0|0.0|0.0|    (4,[1],[1.0])|\n",
      "|Yes| young| true| true|     fair|  0.0|0.0|1.0|1.0|1.0|[0.0,1.0,1.0,1.0]|\n",
      "| No| young|false|false|     fair|  1.0|0.0|0.0|0.0|1.0|    (4,[3],[1.0])|\n",
      "| No|middle|false|false|     fair|  1.0|1.0|0.0|0.0|1.0|[1.0,0.0,0.0,1.0]|\n",
      "| No|middle|false|false|     good|  1.0|1.0|0.0|0.0|0.0|    (4,[0],[1.0])|\n",
      "|Yes|middle| true| true|     good|  0.0|1.0|1.0|1.0|0.0|[1.0,1.0,1.0,0.0]|\n",
      "|Yes|middle|false| true|excellent|  0.0|1.0|0.0|1.0|2.0|[1.0,0.0,1.0,2.0]|\n",
      "|Yes|middle|false| true|excellent|  0.0|1.0|0.0|1.0|2.0|[1.0,0.0,1.0,2.0]|\n",
      "|Yes|   old|false| true|excellent|  0.0|2.0|0.0|1.0|2.0|[2.0,0.0,1.0,2.0]|\n",
      "|Yes|   old|false| true|     good|  0.0|2.0|0.0|1.0|0.0|[2.0,0.0,1.0,0.0]|\n",
      "|Yes|   old| true|false|     good|  0.0|2.0|1.0|0.0|0.0|[2.0,1.0,0.0,0.0]|\n",
      "|Yes|   old| true|false|excellent|  0.0|2.0|1.0|0.0|2.0|[2.0,1.0,0.0,2.0]|\n",
      "| No|   old|false|false|     fair|  1.0|2.0|0.0|0.0|1.0|[2.0,0.0,0.0,1.0]|\n",
      "+---+------+-----+-----+---------+-----+---+---+---+---+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainDf=df2.select('label','features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|label|         features|\n",
      "+-----+-----------------+\n",
      "|  1.0|    (4,[3],[1.0])|\n",
      "|  1.0|        (4,[],[])|\n",
      "|  0.0|    (4,[1],[1.0])|\n",
      "|  0.0|[0.0,1.0,1.0,1.0]|\n",
      "|  1.0|    (4,[3],[1.0])|\n",
      "|  1.0|[1.0,0.0,0.0,1.0]|\n",
      "|  1.0|    (4,[0],[1.0])|\n",
      "|  0.0|[1.0,1.0,1.0,0.0]|\n",
      "|  0.0|[1.0,0.0,1.0,2.0]|\n",
      "|  0.0|[1.0,0.0,1.0,2.0]|\n",
      "|  0.0|[2.0,0.0,1.0,2.0]|\n",
      "|  0.0|[2.0,0.0,1.0,0.0]|\n",
      "|  0.0|[2.0,1.0,0.0,0.0]|\n",
      "|  0.0|[2.0,1.0,0.0,2.0]|\n",
      "|  1.0|[2.0,0.0,0.0,1.0]|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, (4,[3],[1.0])),\n",
       " LabeledPoint(1.0, (4,[],[])),\n",
       " LabeledPoint(0.0, (4,[1],[1.0])),\n",
       " LabeledPoint(0.0, [0.0,1.0,1.0,1.0]),\n",
       " LabeledPoint(1.0, (4,[3],[1.0])),\n",
       " LabeledPoint(1.0, [1.0,0.0,0.0,1.0]),\n",
       " LabeledPoint(1.0, (4,[0],[1.0])),\n",
       " LabeledPoint(0.0, [1.0,1.0,1.0,0.0]),\n",
       " LabeledPoint(0.0, [1.0,0.0,1.0,2.0]),\n",
       " LabeledPoint(0.0, [1.0,0.0,1.0,2.0]),\n",
       " LabeledPoint(0.0, [2.0,0.0,1.0,2.0]),\n",
       " LabeledPoint(0.0, [2.0,0.0,1.0,0.0]),\n",
       " LabeledPoint(0.0, [2.0,1.0,0.0,0.0]),\n",
       " LabeledPoint(0.0, [2.0,1.0,0.0,2.0]),\n",
       " LabeledPoint(1.0, [2.0,0.0,0.0,1.0])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "#from pyspark.mllib.util import Vectors\n",
    "\n",
    "trainRdd = trainDf.rdd.map(lambda row: LabeledPoint(row.label,Vectors.fromML(row.features)))\n",
    "#trainRdd = trainDf.rdd.map(lambda row: LabeledPoint(row.label,Vectors(row.features)))\n",
    "trainRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(label=1.0, features=SparseVector(4, {3: 1.0})),\n",
       " Row(label=1.0, features=SparseVector(4, {})),\n",
       " Row(label=0.0, features=SparseVector(4, {1: 1.0})),\n",
       " Row(label=0.0, features=DenseVector([0.0, 1.0, 1.0, 1.0])),\n",
       " Row(label=1.0, features=SparseVector(4, {3: 1.0})),\n",
       " Row(label=1.0, features=DenseVector([1.0, 0.0, 0.0, 1.0])),\n",
       " Row(label=1.0, features=SparseVector(4, {0: 1.0})),\n",
       " Row(label=0.0, features=DenseVector([1.0, 1.0, 1.0, 0.0])),\n",
       " Row(label=0.0, features=DenseVector([1.0, 0.0, 1.0, 2.0])),\n",
       " Row(label=0.0, features=DenseVector([1.0, 0.0, 1.0, 2.0])),\n",
       " Row(label=0.0, features=DenseVector([2.0, 0.0, 1.0, 2.0])),\n",
       " Row(label=0.0, features=DenseVector([2.0, 0.0, 1.0, 0.0])),\n",
       " Row(label=0.0, features=DenseVector([2.0, 1.0, 0.0, 0.0])),\n",
       " Row(label=0.0, features=DenseVector([2.0, 1.0, 0.0, 2.0])),\n",
       " Row(label=1.0, features=DenseVector([2.0, 0.0, 0.0, 1.0]))]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_trainRdd=trainDf.rdd\n",
    "print type(_trainRdd)\n",
    "_trainRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "lrModel = lr.fit(trainDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.50705810019,-5.31916107407,-5.04694958332,-0.351455356638]\n",
      "3.2822908185\n"
     ]
    }
   ],
   "source": [
    "print lrModel.coefficients\n",
    "print lrModel.intercept\n",
    "\n",
    "#feature별로 가중치를 구함\n",
    "#인터세트는 절편을 구한다고 볼수이있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9167192614845543"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*lrModel.coefficients[0]+1*lrModel.coefficients[3]+lrModel.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-6addfb429ecf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model1' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "#from pyspark.mllib.linalg import Vectors,VectorUDT\n",
    "from pyspark.sql import Row\n",
    "test0 = spark.sparkContext.parallelize([Row(features=Vectors.dense(2,0,0,1))]).toDF()\n",
    "result = model1.transform(test0).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        [0,'my dog has flea problems. help please.'],\n",
    "        [1,'maybe not take him to dog park stupid'],\n",
    "        [0,'my dalmation is so cute. I love him'],\n",
    "        [1,'stop posting stupid worthless garbage'],\n",
    "        [0,'mr licks ate my steak how to stop him'],\n",
    "        [1,'quit buying worthless dog food stupid'],\n",
    "        [0,u'우리 강아지 벌레 있어요 도와주세요'],\n",
    "        [0,u'우리 강아지 귀여워 너 사랑해'],\n",
    "        [1,u'강아지 공원 가지마 바보같이'],\n",
    "        [1,u'강아지 음식 구매 마세요 바보같이']\n",
    "    ],\n",
    "    ['cls','sent']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cls: long (nullable = true)\n",
      " |-- sent: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, HashingTF, StopWordsRemover, RegexTokenizer\n",
    "stopwords=list() \n",
    "# 의미없는 단어들 사전을 갱신시켜줌\n",
    "_mystopwords=[u\"나\",u\"너\", u\"우리\"]\n",
    "for e in _mystopwords:\n",
    "    stopwords.append(e)\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"cls\", outputCol=\"label\")\n",
    "regexTok = RegexTokenizer(inputCol=\"sent\", outputCol=\"wordsRegex\", pattern=\"\\\\s+\")#공백으로 스플릿)\n",
    "#tokenizer = Tokenizer(inputCol=\"sent\", outputCol=\"words\")\n",
    "stop = StopWordsRemover(inputCol=\"wordsRegex\", outputCol=\"nostops\")\n",
    "_stopwords=stop.getStopWords()\n",
    "for e in _stopwords:\n",
    "    stopwords.append(e)\n",
    "stop.setStopWords(stopwords)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"nostops\", outputCol=\"features\")\n",
    "pipeline = Pipeline(stages=[labelIndexer,regexTok,stop,hashingTF])\n",
    "model=pipeline.fit(df)\n",
    "trainDf = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------------+\n",
      "|cls|label|            features|\n",
      "+---+-----+--------------------+\n",
      "|  0|  0.0|(262144,[25688,38...|\n",
      "|  1|  1.0|(262144,[55639,71...|\n",
      "|  0|  0.0|(262144,[46165,81...|\n",
      "|  1|  1.0|(262144,[57368,87...|\n",
      "|  0|  0.0|(262144,[69384,88...|\n",
      "|  1|  1.0|(262144,[57368,75...|\n",
      "|  0|  0.0|(262144,[14050,28...|\n",
      "|  0|  0.0|(262144,[28575,57...|\n",
      "|  1|  1.0|(262144,[28575,17...|\n",
      "|  1|  1.0|(262144,[28575,84...|\n",
      "+---+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDf.select('cls','label','features').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, (262144,[25688,38977,75919,87576,239859],[1.0,1.0,1.0,1.0,1.0]))]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "trainRdd = trainDf\\\n",
    "    .rdd\\\n",
    "    .map(lambda row: LabeledPoint(row.label,Vectors.fromML(row.features)))\n",
    "trainRdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import *\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "lrModel = lr.fit(trainDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* summary:  <pyspark.ml.classification.BinaryLogisticRegressionTrainingSummary object at 0x1099cc7d0>\n",
      "* coefficients:  (262144,[14050,25688,28575,38977,46165,55639,57298,57368,69384,71826,75919,81486,84127,87345,87576,88312,103148,104257,121133,121786,158088,163197,173036,175143,186022,186480,202268,220548,228185,239859,243184,244249,245237,250806,251329],[-1.27222161806,-1.0959340667,-0.0755352913925,-1.0959340667,-1.29204386194,1.13289974307,-1.75263828924,1.16070634922,-1.0381881797,1.13289974307,0.376841766526,-1.29204386194,1.19058324228,1.22114617531,-1.0959340667,-1.0381881797,0.842331778855,-1.27222161806,0.842331778855,1.19058324228,-1.75263828924,1.22114617531,1.6328492213,1.13289974307,0.842331778855,-1.29204386194,0.102913872536,-1.0381881797,1.58818076076,-1.0959340667,-1.27222161806,-1.0381881797,1.19058324228,1.6328492213,1.36987615596])\n",
      "* intercept:  -0.596615892431\n"
     ]
    }
   ],
   "source": [
    "print \"* summary: \", lrModel.summary\n",
    "print \"* coefficients: \", lrModel.coefficients\n",
    "print \"* intercept: \", lrModel.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "wcReduceByKey = wc.reduceByKey(add)\n",
    "In [19]:\n",
    "wcGroupByKey = wc.groupByKey().mapValues(sum)\n",
    "In [24]:\n",
    "wcGroupByKey2 = wc.groupByKey().map(lambda (x,v): (x,len(v)))\n",
    "In [32]:\n",
    "wcReduceByKey.sortByKey().take(10)\n",
    "\n",
    "\n",
    "#키별로 섭 워드벡터로 가운팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
